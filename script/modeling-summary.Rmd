---
title: "Modeling Summary"
subtitle: > 
  Example of using <font class=mrgc>bbr.bayes</font> to summarize across a set of fitted models.
image: bbr-strip.png
order: 500
categories: 
- bbr
- model management
fig-cap-location: margin
toc: true
toc-depth: 2
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  cache = TRUE, 
  autodep = TRUE, 
  comment = '.', 
  message = FALSE,
  warning = FALSE
)
```


# Introduction

Model summaries provide a condensed overview of the key decisions made during the model development process. Theyâ€™re quick to compile and take advantage of the easy annotation features of `bbr`, such as the description, notes, and tags, to summarize the important features of key models.

The page demonstrates how to:

* Create and modify model run logs
* Check and compare model tags
* Graphically represent the relationships between models

For a walk-though on how to define, submit, and annotate your models, see the {{< var pages.initial_model >}} page. 

# Tools used

<hr />

## MetrumRG packages
{{< var used.bbr >}}

## CRAN packages
{{< var used.dplyr >}}


# Set up

<hr />

Load required packages and set file paths to your model and figure directories. 

```{r}
library(bbr)
library(bbr.bayes)
library(dplyr)
library(here)
library(tidyverse)
library(yspec)
library(posterior)
library(igraph)

MODEL_DIR <- here("model", "stan","expo")
FIGURE_DIR <- here("deliv", "figure","expo")
```

```{r, echo=FALSE}

# Source some useful functions
source(here("script", "functions-model.R"))

# define model dir and load tags
TAGS <- yaml::read_yaml(here("script", "tags.yaml"))

```

```{r, eval = TRUE, echo=FALSE, include = FALSE}
#Source helper functions for stepping through the rmd files 
source(here::here("script", "functions-rmd-helpers.R"))

#models needed for this script
models <- c("mod0", "mod0a", "mod1", "mod1a", "mod2", "mod3", "mod4")

for (i in 1:length(models)){
  mod_i <- check_model(file.path(MODEL_DIR, models[i]))
}

```

# Modeling Summary

We can also create a run log of all, or a subset, of the models we've fit:

```{r}
log_df <- run_log(MODEL_DIR) %>%
  collapse_to_string(based_on)
```

The return object is a tibble with information about each model, including where the model resides on disk; the model type (`stan` vs `stan_gq`); model description, tags and notes added to the model object; and a field indicating the model on which each model is based.  For our worked example,  a glimpse of  `log_df` looks like this:

```{r}
glimpse(log_df)
```

We can graphically represent the relationships between our models by plotting the network:

```{r}
log_df_subset <- log_df %>%
  filter(model_type=='stan') %>% 
  filter(based_on!='') %>% 
  select(from=based_on, to=run)

model_graph <- graph_from_data_frame(log_df_subset)
```

```{r, fig.cap='Graphical relationship of models.'}
plot(model_graph,layout=layout_as_tree)
```

[run_log()](https://metrumresearchgroup.github.io/bbr/articles/getting-started.html#run-log) provides qualitative information about each model to give an overview of the modeling process.  We can also extract some quantiative measures of model fit and NUTS diagnostics using the `stan_summary_log` function instead of `run_log`.  That collects Stan models from the specified directory and returns a dedicated tibble; alternatively, you can add this information to a run log using `stan_add_summary`.

```{r}
log_df_quant <- log_df %>%
  filter(model_type == 'stan') %>% 
  stan_add_summary()
```

By default, the resulting tibble contains the following fields, in addition to those in the `run_log` object:

```{r, echo=FALSE}
log_df_quant %>%
  select(all_of(setdiff(names(log_df_quant),
                        names(log_df)))) %>%
  glimpse()
```
Additional variables can be added using the `variables` and `summary_fns` arguments.  For example, if we wanted to add the posterior mean and Monte Carlo standard error for `emax`, `tv_ec50` and `lp__`, we would use the following:

```{r}
log_df_quant2 <- log_df %>%
  filter(model_type == 'stan') %>% 
  stan_add_summary(variables = c('emax','tv_ec50',
                                 'lp__'),
                   summary_fns = list('mean','mcse_mean',
                                      'rhat'))
```

The resulting tibble now contains the following variables:

```{r, echo=FALSE}
log_df_quant2 %>%
  select(all_of(setdiff(names(log_df_quant2),
                        names(log_df)))) %>%
  glimpse()
```
We can use information from the run log or run summary to enhance the graphical representation of the model. For example, we might want to highlight models which have a high Rhat value for the log posterior value:

```{r}
subset_to_plot <- log_df_quant2 %>%
  filter(model_type=='stan') %>% 
  filter(based_on!='') %>% 
  select(from=based_on, to=run)

vertex_data <- log_df_quant2 %>%
  filter(model_type=='stan')  %>% 
  select(run,bad_ebfmi,lp___rhat) %>% 
  mutate(`High Rhat` = as.numeric(lp___rhat > 1.01)+1)

model_graph <- graph_from_data_frame(subset_to_plot ,
                                     vertices = vertex_data)
V(model_graph)$color <- categorical_pal(2)[vertex_data$`High Rhat`]
```

```{r, fig.cap='Graphical relationship of models.  Blue nodes represent models with Rhat > 1.01 for the log posterior.'}
plot(model_graph,layout=layout_as_tree)
```

# Other resources
 <hr /> 
 
The following script from the {{< var stan_expo_repo.main >}} is discussed on this page. If you're interested running this code, visit the {{< var pages.about_the_repo >}} page first.

Modeling Summary script: {{< var stan_expo_repo.modeling_summary >}} 
